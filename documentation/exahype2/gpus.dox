/**

@page page_exahype_gpu GPU support (and aggressive vectorisation)

This page discusses how ExaHyPE supports SIMT and SIMD compute kernels.
ExaHyPE uses GPUs in a classic ***offloading***, i.e. GPUs as accelerators. This means that
it takes particular tasks, ships them to the GPU and then ships them
back. This happens under the hood, and neither does data reside on the
accelerator permanently nor is the accelerator used all the time.

The whole approach assumes that there are computations (cell/patch
updates) which can be done without any effect on global variables. It
works if and only if we can ship a task to the GPU, and then get the
solution data for this task back and no other global variabled changes.
We furthermore assume that the computations that fit onto the GPU have
no state. They can use some global, static variables, but they cannot
access the solver's state which can change over time. We rely on code
parts which *have no side effects and do not depend on the solver state*
(minus global variables).

The same argument holds for aggressive vectorisation: Vectorisation 
works best if the computations on a patch do not have any side effects.
Basically, ExaHyPE assumes that GPU's SIMT and CPU's SIMD are two 
realisations of the same pattern.

Working without side-effects might not work for all patches: There are always patches which
evaluate and analyse some global data, or build up global data
structures. In this case, ExaHyPE only offloads the remaining, simple patches
to the GPU.
That is, having a solver that supports patches/cells without side
effects does not mean that all cells have to be side effect-free.



# Compile with GPU support

Configure ExaHyPE with the argument `--with-gpu=xxx` and select an
appropriate GPU programming model. Rebuild the whole Peano core. 
This includes the ExaHyPE libraries.

If you want to profile or optimise your code further, you might want to pick a
particular vendor toolchain. See configure's help on --with-toolchain.
The @ref tarch_logging "logging section" discusses variations of the 
toolchain support.

If you are "only" interested in the most aggressive vectorisation of your
kernels, you can skip this part.


# Enable stateless compute kernels

For the aggressive vectorisation and the offloading, you have to use
an ExaHyPE solver which has support for GPUs and you ahve to instruct it that 
you plan to have patch updates without access to the solver's internal state.

The Finite Volume enclave solvers for example all support GPUs, but the
versions without enclave tasking do not support them. The solver
documentation should clarify which one to select. For most of these
GPU-enabled solvers, it is sufficient to pass an additional flag

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
pde_terms_without_state=True
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


for the FV variants, e.g.) that tells
the solver that we have PDE terms which have no side effect which also
do not need the solver object.
Consult the documentation of the respective solvers. Examples are

- exahype2.solvers.fv.rusanov.GlobalAdaptiveTimeStepWithEnclaveTasking
- yet to be added


## Provide the additional PDE terms without state.

To faciliate the offloading, we have to create alternative versions of
our PDE term functions that can work independent of the solver object
(and in return cannot modify it). Depending on which terms we have,
we'll need stateless versions of the flux, the non-conservative product
and the source term. We'll always needs the eigenvalue function. Per
function, keep the original one and add a second one which is

-   static;

-   accepts an additional flag of type `Offloadable`. This is the last
    argument, i.e. a static version of a function has exactly the same
    arguments as the non-static, default variant but then has one more
    argument. The last argument is solely there to be able to
    distinguish the static version from the normal one, as C++ cannot
    overload w.r.t. static vs. not static. `Offloadable` is
    automatically defined in the abstract base class which 's Python API
    generates.

-   is embedded into

    \#if defined(OpenMPGPUOffloading) \#pragma omp declare target
    \#endif

    my new static function variants

    \#if defined(OpenMPGPUOffloading) \#pragma omp end declare target
    \#endif

    Though you can leave these annotations away if you use SYCL only.

Very often, the standard flux and eigenvalue routines can invoke the
static variants and you can thus eliminate code redundancies. For
example, you might have the normal flux function and a static flux
function with the additional `Offloadable` parameter, but the normal
function just invokes the static cousin.

In many of our codes, we take a function like

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
void examples::exahype2::SSInfall::SSInfall::flux( ... ) {
  logTraceInWith4Arguments( "flux(...)", faceCentre, volumeH, t, normal ); 

  compute something; 

  logTraceOutWith4Arguments( "flux(...)", faceCentre, volumeH, t, normal );
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

and simply split it into two variants:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
void examples::exahype2::SSInfall::SSInfall::flux( ... ) {
  logTraceInWith4Arguments( "flux(...)", faceCentre, volumeH, t, normal ); 
  
  flux(Q,faceCentre,volumeH,t,normal,F,Offloadable::Yes); 
  
  logTraceOutWith4Arguments( "flux(...)", faceCentre, volumeH, t, normal );
}


void examples::exahype2::SSInfall::SSInfall::flux( ..., Offloadable ) {
  compute something;
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The static version is used on the GPU. The normal one is used on the CPU
yet defers immediately to the static version. Please note that GPUs are
quite restrictive w.r.t. terminal outputs and assertions. You will have
to remove both from your static routine versions. In the example above,
the non-static version wraps the core functionality into assertions.
Therefore, we have the assertions on the CPU, but the core part that
goes to the GPU is free of assertions. The same holds for the logging.


Most ExaHyPE solvers allow you to insert the realisation of a routine directly
from the Python code. In this case, the call ends up the AbstractMySolver
class. 
Most classes call the corresponding routine set_implementation(). If you use
this one, the code generator usually does not distinguish the two callback
flavours and uses the same code snippet for the normal as well as the 
vectorised version.
However, it is likely that the files end up in the cpp file of the abstract 
super class. That is, the compiler will not be able to inline anything. If 
your compiler struggles with the inlining, you might be forced either 

- to dive into linker-time optimisation - Intel, for example, did allow you to 
  run interprocedural optimisation (ipo) on bundles of object files; or
- to switch to a manual implementation and to move the GPU routines into the
  headers. 


## Alter compute kernel

Feel free to reset

        self._fused_compute_kernel_call_cpu
        
in your solver class and to switch to a faster kernel realisation variant. 
Such a switch follows the @ref  page_exahype_performance_optimisation "The code does not use all cores" discussion.


## Mask out cells which are not a fit

patchCanUseStatelessPDETerms() yields, by default, true. So all compute kernels
end up on the GPU if they are embedded into enclave tasks. You migth want to 
alter this, and keep some kernels on the host.



# Pick a threading backend which supports GPU offloading.

The default threading model of  does not support task fusion or the
offloading to GPUs. If you run the code with `–help`, it will report on
various threading models. Use one that has `fuse` in its title, and you
should finally get GPU offloading.

If only some patches/cells can be offloaded to the GPU, then you can
redefine the routine

virtual bool patchCanUseStatelessPDETerms( const
tarch::la::Vector\<Dimensions,double\>& patchCentre, const
tarch::la::Vector\<Dimensions,double\>& patchH, double t, double dt )
const;

in your solver. By default, this routine returns `true` always. Here's
the clue: This is a normal function, i.e. you can use the solver's state
and make the result depend on this one.
 
 
  */
  
  