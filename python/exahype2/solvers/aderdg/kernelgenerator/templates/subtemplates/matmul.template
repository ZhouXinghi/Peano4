{#
      C       = alpha  *   A   *    B   + beta  *  C
   (M x N)              (M x K)  (K x N)
  The gemm config (fetched through matmulKey) contains M, N, K, LDA, LDB, LDC, alpha and beta
  See matmulConfig

  String matmulKey : name of the associated config
  String A         : name of A
  String B         : name of B
  String C         : name of C
  String A_shift   : shift to the zero of A
  String B_shift   : shift to the zero of B
  String C_shift   : shift to the zero of C
  
  optional
  bool overrideUseLibxsmm : force locally useLibxsmm to take this value if set
  String trueB            : true array B, B must b a true matrix, not a tensor slice
  String trueAlpha        : true value of the coefficent alpha (note: it will be multiplicated by the configuration alpha, /!\ sign error)
  bool forceCoeffMatrix   : only when using trueB, trueAlpha, force the no libxsmm case to also generate the coeff matrix
  
  If trueB is used, a temporary array trueAlpha*trueB is generated
#}
{% with %}
{# /**************************************
   **** Set up helper template values ****
   **************************************/ #}
{% set conf = matmulConfigs[matmulKey] %}
{% if overrideUseLibxsmm is not defined or overrideUseLibxsmm == "BoolNotDefined" %}
  {% set overrideUseLibxsmm = useLibxsmm %}{# if no override then take the current value #}
{% endif %}
{% if overrideUseBLIS is not defined or overrideUseBLIS == "BoolNotDefined" %}
  {% set overrideUseBLIS = useBLIS %}{# if no override then take the current value #}
{% endif %}
{% if overrideUseEigen is not defined or overrideUseEigen == "BoolNotDefined" %}
  {% set overrideUseEigen = useEigen %}{# if no override then take the current value #}
{% endif %}
{% if overrideUseLibxsmmJIT is not defined or overrideUseLibxsmmJIT == "BoolNotDefined" %}
  {% set overrideUseLibxsmmJIT = useLibxsmmJIT %}{# if no override then take the current value #}
{% endif %}
{% if trueB is not defined or trueB == "" %}
  {% set trueB = B %}
  {% set useTrueB = False %}
{% else %}
  {% set useTrueB = True %}
{% endif %}
{% if forceCoeffMatrix is not defined %}
  {% set forceCoeffMatrix = False %}
{% endif %}
{# set arrays' name for pragma by removing eventual index #}
{% set Ap = (A.split("["))[0] %}
{% set Bp = (B.split("["))[0] %}
{% set Cp = (C.split("["))[0] %}
{% set trueBp = (trueB.split("["))[0] %}
{% set M = matmulConfigs[matmulKey].M %}
{% set N = matmulConfigs[matmulKey].N %}
{% set K = matmulConfigs[matmulKey].K %}
{% set LDA = matmulConfigs[matmulKey].LDA %}
{% set LDB = matmulConfigs[matmulKey].LDB %}
{% set LDC = matmulConfigs[matmulKey].LDC %}
{% set alpha = matmulConfigs[matmulKey].alpha %}
{% set beta = matmulConfigs[matmulKey].beta %}
{% set precision = matmulConfigs[matmulKey].precision[0] %}
{% if precision == "DP" %}
{% set precisionPrefix = "d" %}
{% set precisionType = "double" %}
{% elif precision== "SP" %}
{% set precisionPrefix = "s" %}
{% set precisionType = "float" %}
{% endif %}
{% if matmulConfigs[matmulKey].alignment_A == 1 %}
{% set alignmentA = "Aligned" %}
{% else %}
{% set alignmentA = "Unaligned" %}
{% endif %}
{% if matmulConfigs[matmulKey].alignment_C == 1 %}
{% set alignmentC = "Aligned" %}
{% else %}
{% set alignmentC = "Unaligned" %}
{% endif %}
{% if matmulConfigs[matmulKey].beta == 1 %}
{% set eigenOp = "+=" %}
{% else %}
{% set eigenOp = "=" %}
{% endif %}
{# /********************
   **** Subtemplate ****
   *********************/ #}

{#

// BLIS case
//-------------

#}
{% if overrideUseBLIS %}
{% if useTrueB %}{# will set B[it] to be trueAlpha*trueB[it] #}
pCompType {{B}}[{{conf.LDB*conf.K}}] __attribute__((aligned({{alignmentSize}})));
#pragma omp simd aligned({{Bp}},{{trueBp}}:{{alignmentSize}})
for (int it = 0; it < {{conf.LDB*conf.K}}; it++) {
  {{B}}[it] = {{trueAlpha}} * {{trueB}}[it];
}
#if defined(USE_IPO) && !defined(UNSAFE_IPO)
volatile double doNotOptimizeAway_{{B}} = {{B}}[0]; //used to prevent the compiler from optimizing temp array away. Needs to be volatile
#endif
{% endif %}{# useTrueB #}
#ifdef USE_IPO
#pragma forceinline
#endif
bli_{{precisionPrefix}}gemm(BLIS_NO_TRANSPOSE, BLIS_NO_TRANSPOSE, {{M}}, {{N}}, {{K}}, {% if alpha == 1 %}&one_{{precisionPrefix}}{% elif alpha == 0 %}&zero_{{precisionPrefix}}{% endif %}, {{A}}{% if A_shift != '0' %}+{{A_shift}}{% endif %}, 1, {{LDA}}, {{B}}{% if B_shift != '0' %}+{{B_shift}}{% endif %}, 1, {{LDB}}, {% if beta == 1 %}&one_{{precisionPrefix}}{% elif beta == 0 %}&zero_{{precisionPrefix}}{% endif %}, {{C}}{% if C_shift != '0' %}+{{C_shift}}{% endif %}, 1, {{LDC}});
{#

// Eigen case
//-------------
// TODO: use the actual alignment values for the Maps

#}
{% elif overrideUseEigen %}
{% if useTrueB %}{# will set B[it] to be trueAlpha*trueB[it] #}
pCompType {{B}}[{{conf.LDB*conf.K}}] __attribute__((aligned({{alignmentSize}})));
#pragma omp simd aligned({{Bp}},{{trueBp}}:{{alignmentSize}})
for (int it = 0; it < {{conf.LDB*conf.K}}; it++) {
  {{B}}[it] = {{trueAlpha}} * {{trueB}}[it];
}
#if defined(USE_IPO) && !defined(UNSAFE_IPO)
volatile double doNotOptimizeAway_{{B}} = {{B}}[0]; //used to prevent the compiler from optimizing temp array away. Needs to be volatile
#endif
{% endif %}{# useTrueB #}
#ifdef USE_IPO
#pragma forceinline
#endif
{
    Eigen::Map<Eigen::Matrix<{{precisionType}}, {{M}}, {{K}}>, Eigen::Unaligned, Eigen::Stride<{{LDA}}, 1>> A({{A}}{% if A_shift != '0' %}+{{A_shift}}{% endif %});
    Eigen::Map<Eigen::Matrix<{{precisionType}}, {{K}}, {{N}}>, Eigen::Unaligned, Eigen::Stride<{{LDB}}, 1>> B({{B}}{% if B_shift != '0' %}+{{B_shift}}{% endif %});
    Eigen::Map<Eigen::Matrix<{{precisionType}}, {{M}}, {{N}}>, Eigen::Unaligned, Eigen::Stride<{{LDC}}, 1>> C({{C}}{% if C_shift != '0' %}+{{C_shift}}{% endif %});
    C {{eigenOp}} {% if alpha != 1%}{{alpha}} * (A * B){% else %}A * B{% endif %};
}
{#

// LIBXSMM case
//-------------

#}
{% elif overrideUseLibxsmm or overrideUseLibxsmmJIT %}
{% if useTrueB %}{# will set B[it] to be trueAlpha*trueB[it] #}
pCompType {{B}}[{{conf.LDB*conf.K}}] __attribute__((aligned({{alignmentSize}})));
#pragma omp simd aligned({{Bp}},{{trueBp}}:{{alignmentSize}})
for (int it = 0; it < {{conf.LDB*conf.K}}; it++) {
  {{B}}[it] = {{trueAlpha}} * {{trueB}}[it];
}
#if defined(USE_IPO) && !defined(UNSAFE_IPO)
volatile double doNotOptimizeAway_{{B}} = {{B}}[0]; //used to prevent the compiler from optimizing temp array away. Needs to be volatile
#endif
{% endif %}{# useTrueB #}
#ifdef USE_IPO
#pragma forceinline
#endif
{{conf.baseroutinename}}({{A}}{% if A_shift != '0' %}+{{A_shift}}{% endif %}, {{B}}{% if B_shift != '0' %}+{{B_shift}}{% endif %}, {{C}}{% if C_shift != '0' %}+{{C_shift}}{% endif %});
{% else %}
{# 

// No LIBXSMM case
//----------------

#}
{% if forceCoeffMatrix %}
double {{B}}[{{conf.LDB*conf.K}}] __attribute__((aligned({{alignmentSize}})));
#pragma omp simd aligned({{Bp}},{{trueBp}}:{{alignmentSize}})
for (int it = 0; it < {{conf.LDB*conf.K}}; it++) {
  {{B}}[it] = {{trueAlpha}} * {{trueB}}[it];
}
{% set trueB = B %}
{% endif %}
{% if conf.beta == 0 %}
// reset {{C}}
for (int it_1 = 0; it_1 < {{conf.N}}; it_1++) {
  #pragma omp simd aligned({{Cp}}:{{alignmentSize}})
  for (int it_3 = 0; it_3 < {{conf.M}}; it_3++) {
    {{C}}[{{C_shift}}+it_1*{{conf.LDC}}+it_3] = 0.;
  }
}
{% endif %}
for (int it_1 = 0; it_1 < {{conf.N}}; it_1++) {
  for (int it_2 = 0; it_2 < {{conf.K}}; it_2++) {
    #pragma omp simd aligned({{Cp}},{{Ap}},{{trueBp}}:{{alignmentSize}})
    for (int it_3 = 0; it_3 < {{conf.M}}; it_3++) {
      {{C}}[{{C_shift}}+it_1*{{conf.LDC}}+it_3] {{ '+' if conf.alpha == 1 else '-' }}= {{A}}[{{A_shift}}+it_2*{{conf.LDA}}+it_3] * {% if (useTrueB and not forceCoeffMatrix) %}{{trueAlpha}}* {% endif %}{{trueB}}[{% if B_shift != '0' %}{{B_shift}}+{% endif %}it_1*{{conf.LDB}}+it_2];
    }
  }
}
{% endif %}
{% endwith %}