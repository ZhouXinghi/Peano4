// **********************************************************************************************
// ***                                     !!!WARNING!!!                                      ***
// *** WARNING: AUTO GENERATED FILE! DO NOT MODIFY BY HAND! YOUR CHANGES WILL BE OVERWRITTEN! ***
// ***                                     !!!WARNING!!!                                      ***
// ***                  Generated by Peano's Python API: www.peano-framework.org              ***
// **********************************************************************************************
#pragma once

#include "peano4/datamanagement/CellMarker.h"
#include "tarch/multicore/Tasks.h"
#include "tarch/NonCriticalAssertions.h"

#include "exahype2/fv/rusanov/rusanov.h"
#include "exahype2/EnclaveBookkeeping.h"
#include "exahype2/EnclaveTask.h"
#include "repositories/SolverRepository.h"

#include <vector>
#include <memory>

#ifdef UseSmartMPI
#include "smartmpi.h"
#endif

{% for item in NAMESPACE -%}
  namespace {{ item }} {

{%- endfor %}
  class {{CLASSNAME}};

{% for item in NAMESPACE -%}
  }
{%- endfor %}


/**
 * Single task that can also take multiple tasks and deploy them to the GPU
 *
 * @author Tobias Weinzierl
 */
class {{NAMESPACE | join("::")}}::{{CLASSNAME}}: public ::exahype2::EnclaveTask
#ifdef UseSmartMPI
, public smartmpi::Task
#endif
{
  private:
    static tarch::logging::Log _log;

#ifdef UseSmartMPI
    /**
     * This uniquely identifies an enclave task. Unlike _enclaveTaskTypeId it
     * differentiates between tasks within a task type, not between task types.
     *
     * Crucially, the value of this ID is not necessarily the same as that of
     * tarch::multicore::Task::_id . That is because _id is reset each time
     * we move a task to another rank and reconstitute it there (see the constructor
     * of tarch::multicore::Task). Instead, _remoteTaskId tracks the ID of
     * the original task object (i.e. the task originally spawned and
     * only then moved). In smartmpi we need to keep track of the task's
     * ID so that it can be bookmarked correctly after being moved around.
     *
     * As such moveTask(...), sends _id if the task has not yet been
     * moved and _remoteTaskId if it has already been moved. Similarly,
     * _remoteTaskId is always sent when forwarding task outcomes since the
     * task will already have been moved.
     */
    int          _remoteTaskId = -1;
#endif

    std::shared_ptr< double[] >                 _linearCombinationOfPreviousShots;

    /**
     * This is a unique static id which we use both for task fusion and within
     * SmartMPI to distinguish different task types.
     */
    static int _enclaveTaskTypeId;

  public:
    static int getEnclaveTaskTypeId();

    /**
     * Simple call to compute kernels. No data is allocated or freed.
     *
     * ## Difference to Finite Volume solver
     *
     * The finite volume solver assumes that _inputValues are reconstructed
     * values, i.e. a temporary data field on the heap. So we don't have to
     * allocate anything but we have to free stuff after the run.
     *
     * For DG, things are different: Here, we do not operate on a guess but
     * actually on a given solution which is persistently stored. All the
     * guesses will eventually be combined into one linear combination
     * yieldign the new time step solution. So we have to keep it.
     * Consequently, there's no need for us to delete anything.
     */
    static void applyKernelToCell(
      const ::peano4::datamanagement::CellMarker& marker,
      double                                      timeStamp,
      double                                      timeStepSize,
      double* __restrict__                        QIn,
      double* __restrict__                        QOut
    );

    /**
     * Initialise superclass with right compute functor
     *
     * Besides the "configuration" of the superclass, we have to copy the
     * QIn data from the marker manually: The action set spawning the task
     * will have constructed the input data. In the Runge-Kutta world, this
     * is a linear combination of previous shots and the old time step's
     * solution. However, this linear combination is not stored persistently
     * and hence might be lost by the time we invoke the enclave task. Hence
     * the copy.
     *
     * We initialise the superclass with a new chunk for memory, and we fill
     * this information with proper content in the subclass constructor. To
     * facilitate GPU offloading with managed memory, we allocate this new
     * copy of the input data in managed memory and rely on tarch::allocateMemory()
     * to place it on the heap if no managed memory is available.
     *
     * The superclass will take care of a proper allocation and handling of
     * the output data. See the class description of EnclaveTask for details.
     * As we create these manual input copies and as applyKernelToCell()
     * does not free anything, our task functor not only calls
     * applyKernelToCell(). It also has to free the input data.
     *
     * ## Remarks on tree traversals using smart pointers
     *
     * If we have a tree traversal which uses smart pointers, it means that the
     * mesh moves around pointers, but the actual data resides at the same
     * place in memory. Consequently, the volume integral solve can write
     * directly into the output. The fact that this is ok is indicated by
     * the output argument. If it equals nullptr, we have to create our own
     * data on the heap to store the output and hand that one over to the
     * enclave solver. If output is however properly set, this is the flag
     * that we can directly write into the output.
     *
     * I originally thought that I could use the same rationale to avoid to 
     * copy the input data as well. However, this does not work. I want to 
     * give the traversal the opportunity to throw away the linear combination
     * after each mesh traversal. So I have to copy the data in this case as
     * well. It is not that dramatic as with the output: 
     * 
     * We may assume that the tasks are pending only relatively shortly, so 
     * the data will be freed quickly again. It is not like the output which
     * will stay alive for almost a whole mesh traversal.
     * 
     * See other constructor which is to be used if we employ smart pointers.
     *
     * @see applyKernelToCell()
     * @param output Pointer to output data structure. We pass that one through
     *   to the superclass. According to the definition there, a nullptr is ok,
     *   in which case the superclass will prepare the output data structure
     *   and eventually tell the enclave bookkeeping to free it again.
     * @param linearCombinationOfPreviousShots This is a the linear combination
     *   over the previous Runge-Kutta shots subject to the weightings from the
     *   Butcher tableau. If output is nullptr, we know that the enclave
     *   solver may not write directly into the output. This is likely as the
     *   data within the mesh is moved around. In this case, we may also not
     *   use linearCombinationOfPreviousShots later within the task but rather
     *   have to create a copy. See description above.
     */
    {{CLASSNAME}}(
      const ::peano4::datamanagement::CellMarker& marker,
      double                                      timeStamp,
      double                                      timeStepSize,
      const double* __restrict__                  linearCombinationOfPreviousShots
    );
    
    {{CLASSNAME}}(
      const ::peano4::datamanagement::CellMarker& marker,
      double                                      timeStamp,
      double                                      timeStepSize,
      std::shared_ptr< double[] >                 linearCombinationOfPreviousShots,
      double* __restrict__                        output
    );

#ifdef UseSmartMPI
    bool isSmartMPITask() const;
    virtual void runLocally() override;
    virtual void moveTask(int rank, int tag, MPI_Comm communicator) override;
    virtual void runLocallyAndSendTaskOutputToRank(int rank, int tag, MPI_Comm communicator) override;
    virtual void forwardTaskOutputToRank(int rank, int tag, MPI_Comm communicator) override;

    static smartmpi::Task* receiveTask(int rank, int tag, MPI_Comm communicator);
    static smartmpi::Task* receiveOutcome(int rank, int tag, MPI_Comm communicator, const bool intentionToForward);
#endif

    {% if STATELESS_PDE_TERMS %}
    virtual bool fuse( const std::list<Task*>& otherTasks, int targetDevice=Host ) override;
    virtual bool canFuse() const override;
    {% endif %}
};

{# Empty line here #}
