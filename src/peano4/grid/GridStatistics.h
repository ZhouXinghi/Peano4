
//
// Generated by DaStGen2 (C) 2020 Tobias Weinzierl
//
// For DaStGen's copyright, visit www.peano-framework.org. These generated files
// however are not subject of copyright, i.e. feel free to add your copyright in
// here
//
#pragma once

#include <string>

#ifdef Parallel
#include <mpi.h>
#include <functional>
#endif

#include "tarch/la/Vector.h"
#include "tarch/mpi/Rank.h"
#include "tarch/services/ServiceRepository.h"
#include "peano4/grid/LoadStoreComputeFlag.h"
#include "peano4/utils/Globals.h"
#include "peano4/grid/TraversalObserver.h"



namespace peano4{
namespace grid{
  struct GridStatistics;
}
}

struct peano4::grid::GridStatistics {
  public:
    GridStatistics() {}
    GridStatistics(int  __numberOfLocalUnrefinedCells, int  __numberOfRemoteUnrefinedCells, int  __numberOfLocalRefinedCells, int  __numberOfRemoteRefinedCells, int  __stationarySweeps, bool  __coarseningHasBeenVetoed, bool  __removedEmptySubtree, tarch::la::Vector<Dimensions,double>  __minH);

    int   getNumberOfLocalUnrefinedCells() const;
    void   setNumberOfLocalUnrefinedCells(int value);
    int   getNumberOfRemoteUnrefinedCells() const;
    void   setNumberOfRemoteUnrefinedCells(int value);
    int   getNumberOfLocalRefinedCells() const;
    void   setNumberOfLocalRefinedCells(int value);
    int   getNumberOfRemoteRefinedCells() const;
    void   setNumberOfRemoteRefinedCells(int value);
    int   getStationarySweeps() const;
    void   setStationarySweeps(int value);
    bool   getCoarseningHasBeenVetoed() const;
    void   setCoarseningHasBeenVetoed(bool value);
    bool   getRemovedEmptySubtree() const;
    void   setRemovedEmptySubtree(bool value);
    tarch::la::Vector<Dimensions,double>   getMinH() const;
    void   setMinH(const tarch::la::Vector<Dimensions,double>& value);
    double   getMinH(int index) const;
    void   setMinH(int index, double value);
    GridStatistics(const GridStatistics& copy) = default;


    #ifdef Parallel
    /**
     * Hands out MPI datatype if we work without the LLVM MPI extension.
     * If we work with this additional feature, this is the routine where
     * the lazy initialisation is done and the datatype is also cached.
     */
    /*[[clang::map_mpi_datatype]]*/
    static MPI_Datatype  getForkDatatype();

    /*[[clang::map_mpi_datatype]]*/
    static MPI_Datatype  getJoinDatatype();

    /*[[clang::map_mpi_datatype]]*/
    static MPI_Datatype  getBoundaryExchangeDatatype();

    /*[[clang::map_mpi_datatype]]*/
    static MPI_Datatype  getMultiscaleDataExchangeDatatype();

    /*[[clang::map_mpi_datatype]]*/
    static MPI_Datatype  getGlobalCommunciationDatatype();

    /*[[clang::map_mpi_datatype]]*/
    static void  freeForkDatatype();

    /*[[clang::map_mpi_datatype]]*/
    static void  freeJoinDatatype();

    /*[[clang::map_mpi_datatype]]*/
    static void  freeBoundaryExchangeDatatype();

    /*[[clang::map_mpi_datatype]]*/
    static void  freeMultiscaleDataExchangeDatatype();

    /*[[clang::map_mpi_datatype]]*/
    static void  freeGlobalCommunciationDatatype();

    /**
     * @return The rank of the sender of an object. It only make ssense to call
     *         this routine after you've invoked receive with MPI_ANY_SOURCE.
     */
    int getSenderRank() const;

    /**
     * Wrapper around getDatatype() to trigger lazy evaluation if we
     * use the lazy initialisation.
     */
    static void initDatatype();

    /**
     * Free the underlying MPI datatype.
     */
    static void shutdownDatatype();

    /**
     * In DaStGen (the first version), I had a non-static version of the send
     * as well as the receive. However, this did not work with newer C++11
     * versions, as a member function using this as pointer usually doesn't
     * see the vtable while the init sees the object from outside, i.e.
     * including a vtable. So this routine now is basically an alias for a
     * blocking MPI_Send.
     */
    static void send(const peano4::grid::GridStatistics& buffer, int destination, int tag, MPI_Comm communicator );
    static void receive(peano4::grid::GridStatistics& buffer, int source, int tag, MPI_Comm communicator );

    /**
     * Alternative to the other send() where I trigger a non-blocking send an
     * then invoke the functor until the corresponding MPI_Test tells me that
     * the message went through. In systems with heavy MPI usage, this can
     * help to avoid deadlocks.
     */
    static void send(const peano4::grid::GridStatistics& buffer, int destination, int tag, std::function<void()> startCommunicationFunctor, std::function<void()> waitFunctor, MPI_Comm communicator );
    static void receive(   peano4::grid::GridStatistics& buffer, int source,      int tag, std::function<void()> startCommunicationFunctor, std::function<void()> waitFunctor, MPI_Comm communicator );
    #endif


    enum ObjectConstruction {
      NoData
    };

    GridStatistics( ObjectConstruction ):
        GridStatistics() {}
    
#ifdef Parallel
    static void sendAndPollDanglingMessages(const peano4::grid::GridStatistics& message, int destination, int tag, MPI_Comm communicator=tarch::mpi::Rank::getInstance().getCommunicator());
    static void receiveAndPollDanglingMessages(peano4::grid::GridStatistics& message, int source, int tag, MPI_Comm communicator=tarch::mpi::Rank::getInstance().getCommunicator() );
#endif
    

    std::string toString() const;

  private:
    /*[[clang::pack_range(0,std::numeric_limits<int>::max())]]*/  int   _numberOfLocalUnrefinedCells;
    /*[[clang::pack_range(0,std::numeric_limits<int>::max())]]*/  int   _numberOfRemoteUnrefinedCells;
    /*[[clang::pack_range(0,std::numeric_limits<int>::max())]]*/  int   _numberOfLocalRefinedCells;
    /*[[clang::pack_range(0,std::numeric_limits<int>::max())]]*/  int   _numberOfRemoteRefinedCells;
    /*[[clang::pack_range(0,std::numeric_limits<int>::max())]]*/  int   _stationarySweeps;
    /*[[clang::pack]]*/  bool   _coarseningHasBeenVetoed;
    /*[[clang::pack]]*/  bool   _removedEmptySubtree;
    tarch::la::Vector<Dimensions,double>   _minH;

    #ifdef Parallel
    private:
      int _senderDestinationRank;

      #if !defined(__MPI_ATTRIBUTES_LANGUAGE_EXTENSION__)
      /**
       * Whenever we use LLVM's MPI extension (DaStGe), we rely on lazy
       * initialisation of the datatype. However, Peano calls init explicitly
       * in most cases. Without the LLVM extension which caches the MPI
       * datatype once constructed, this field stores the type.
       */
      static MPI_Datatype  Datatype;
      #endif
    #endif
};
