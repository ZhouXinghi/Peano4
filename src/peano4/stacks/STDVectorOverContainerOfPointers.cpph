template <class T>
tarch::logging::Log peano4::stacks::STDVectorOverContainerOfPointers<T>::_log("peano4::stacks::"
                                                                              "STDVectorOverContainerOfPointers<T>");


#ifdef Parallel

template <class T>
void peano4::stacks::STDVectorOverContainerOfPointers<T>::workInReceivedMetaData() {
  assertion(_metaDataSizeBuffer != nullptr);
#if PeanoDebug >= 1
  assertion(_metaDataDebugBuffer != nullptr);
#endif

  for (int i = 0; i < Base::_currentElement; i++) {
#if PeanoDebug >= 1 and Dimensions == 2
    tarch::la::Vector<Dimensions, double> x = {
      _metaDataDebugBuffer[i * 2 * 2 + 0], _metaDataDebugBuffer[i * 2 * 2 + 1]};
    tarch::la::Vector<Dimensions, double> h = {
      _metaDataDebugBuffer[i * 2 * 2 + 2], _metaDataDebugBuffer[i * 2 * 2 + 3]};
    Base::_data[i].setDebugX(x);
    Base::_data[i].setDebugH(h);
#endif
#if PeanoDebug >= 1 and Dimensions == 3
    tarch::la::Vector<Dimensions, double> x = {
      _metaDataDebugBuffer[i * 2 * 3 + 0], _metaDataDebugBuffer[i * 2 * 3 + 1], _metaDataDebugBuffer[i * 2 * 3 + 2]};
    tarch::la::Vector<Dimensions, double> h = {
      _metaDataDebugBuffer[i * 2 * 3 + 3], _metaDataDebugBuffer[i * 2 * 3 + 4], _metaDataDebugBuffer[i * 2 * 3 + 5]};
    Base::_data[i].setDebugX(x);
    Base::_data[i].setDebugH(h);
#endif
    assertion( _metaDataSizeBuffer[i]>=0 );
    assertionEquals(Base::_data[i].size(), 0);
    logDebug("workInReceivedMetaData()", "initialised entry " << i << ": " << Base::_data[i].toString());
  }
}


template <class T>
void peano4::stacks::STDVectorOverContainerOfPointers<T>::workInReceivedData() {
  int entryInFlattenedDeepCopyBuffer = 0;
  for (int i = 0; i < Base::_currentElement; i++) {
    assertionEquals(Base::_data[i].size(), 0);
    for (int entryForThisStackElement = 0; entryForThisStackElement < _metaDataSizeBuffer[i]; entryForThisStackElement++) {
      Base::_data[i].addParticle(new typename T::DoFType(_deepCopyDataBuffer[entryInFlattenedDeepCopyBuffer]));
      entryInFlattenedDeepCopyBuffer++;
    }
  }
  logWarning( "workInReceivedData()", "worked in " << entryInFlattenedDeepCopyBuffer << " entries in total" );
}


template <class T>
void peano4::stacks::STDVectorOverContainerOfPointers<T>::prepareDataToSendOut() {
  int entryInFlattenedDeepCopyBuffer = 0;
  for (int i = 0; i < Base::_currentElement; i++) {
    for (auto& p : Base::_data[i]) {
      _deepCopyDataBuffer[entryInFlattenedDeepCopyBuffer] = *p;
      entryInFlattenedDeepCopyBuffer++;
    }
  }
}


template <class T>
int peano4::stacks::STDVectorOverContainerOfPointers<T>::prepareMetaDataToSendOut() {
  int flattenedDataSize = 0;
  for (int i = 0; i < Base::_currentElement; i++) {
    int sizeOfThisStackEntry = Base::_data[i].size();
    flattenedDataSize += sizeOfThisStackEntry;
    _metaDataSizeBuffer[i] = sizeOfThisStackEntry;
#if PeanoDebug >= 1 and Dimensions == 2
    _metaDataDebugBuffer[i * 2 * 2 + 0] = Base::_data[i].getDebugX()[0];
    _metaDataDebugBuffer[i * 2 * 2 + 1] = Base::_data[i].getDebugX()[1];
    _metaDataDebugBuffer[i * 2 * 2 + 2] = Base::_data[i].getDebugH()[0];
    _metaDataDebugBuffer[i * 2 * 2 + 3] = Base::_data[i].getDebugH()[1];
#endif
#if PeanoDebug >= 1 and Dimensions == 3
    _metaDataDebugBuffer[i * 2 * 3 + 0] = Base::_data[i].getDebugX()[0];
    _metaDataDebugBuffer[i * 2 * 3 + 1] = Base::_data[i].getDebugX()[1];
    _metaDataDebugBuffer[i * 2 * 3 + 2] = Base::_data[i].getDebugX()[2];
    _metaDataDebugBuffer[i * 2 * 3 + 3] = Base::_data[i].getDebugH()[0];
    _metaDataDebugBuffer[i * 2 * 3 + 4] = Base::_data[i].getDebugH()[1];
    _metaDataDebugBuffer[i * 2 * 3 + 5] = Base::_data[i].getDebugH()[2];
#endif
    logDebug("prepareMetaDataToSendOut()", "prepare entry " << i << ": " << Base::_data[i].toString());
  }
  return flattenedDataSize;
}

template <class T>
void peano4::stacks::STDVectorOverContainerOfPointers<T>::triggerNonBlockingDataReceive() {
  assertion(_metaDataSizeBuffer != nullptr);
  assertion(_metaDataSizeMPIRequest != nullptr);

  int flattenedDataSize = 0;
  assertionEquals(Base::_data.size(), Base::_currentElement);
  for (int i = 0; i < Base::_currentElement; i++) {
    flattenedDataSize += _metaDataSizeBuffer[i];
  }

  if (flattenedDataSize > 0) {
    Base::_ioMPIRequest = new MPI_Request;
    _deepCopyDataBuffer = new typename T::DoFType[flattenedDataSize];
    MPI_Irecv(
      _deepCopyDataBuffer,
      flattenedDataSize,
      translateContextIntoDatatype<typename T::DoFType>(_context),
      Base::_ioRank,
      Base::_ioTag,
      _communicator,
      Base::_ioMPIRequest
    );
    logDebug(
      "startReceive(int,int,int)", "trigger receive of " << flattenedDataSize << " entries of user data in total"
    );
  } else {
    logDebug("startReceive(int,int,int)", "there's no actual user data to received");
  }
}

#endif


template <class T>
peano4::stacks::STDVectorOverContainerOfPointers<T>::STDVectorOverContainerOfPointers() {
#ifdef Parallel
  _metaDataSizeMPIRequest  = nullptr;
  _metaDataDebugMPIRequest = nullptr;
  Base::_ioMPIRequest      = nullptr;

  _deepCopyDataBuffer  = nullptr;
  _metaDataSizeBuffer  = nullptr;
  _metaDataDebugBuffer = nullptr;
#endif
}


template <class T>
void peano4::stacks::STDVectorOverContainerOfPointers<T>::clone(const STDVectorOverContainerOfPointers<T>& data) {
  assertionEquals1(Base::_currentElement, 0, Base::toString());

  Base::_data.clear();
  Base::_currentElement = data._currentElement;
  Base::_data.resize(Base::_currentElement, T(T::ObjectConstruction::NoData));

  logDebug("clone()", "copy over stack of size " << data.size());

  for (int stackNumber = 0; stackNumber < Base::_currentElement; stackNumber++) {
    Base::_data[stackNumber].clone(data._data[stackNumber]);
    logDebug("clone()", "- copied " << Base::_data[stackNumber].size());
  }
}


template <class T>
void peano4::stacks::STDVectorOverContainerOfPointers<T>::clear() {
  logDebug("clear()", "clear " << Base::_currentElement << " vectors on current stack");
  for (auto& p : Base::_data) {
    p.clear();
  }
  Base::_currentElement = 0;
}


template <class T>
void peano4::stacks::STDVectorOverContainerOfPointers<T>::startSend(
  peano4::grid::TraversalObserver::SendReceiveContext context, int rank, int tag, MPI_Comm comm
) {
#ifdef Parallel
  assertion(Base::_ioMode == IOMode::None);

  Base::_ioMode = IOMode::MPISend;
  Base::_ioTag  = tag;
  Base::_ioRank = rank;

  assertion(_metaDataSizeMPIRequest == nullptr);
  assertion(_metaDataDebugMPIRequest == nullptr);
  assertion(Base::_ioMPIRequest == nullptr);

  assertion(_deepCopyDataBuffer == nullptr);
  assertion(_metaDataSizeBuffer == nullptr);
  assertion(_metaDataDebugBuffer == nullptr);

  logDebug("startSend(int,int)", toString());

  _metaDataSizeBuffer = new int[Base::_currentElement];
#if PeanoDebug >= 1
  _metaDataDebugBuffer = new double[Base::_currentElement * 2 * Dimensions];
#endif

  int flattenedDataSize = prepareMetaDataToSendOut();
  logDebug(
    "startSend(int,int)",
    "assembled and flattened meta data encoding entries per stack entry. Total number of entries is "
      << flattenedDataSize
  );

  _metaDataSizeMPIRequest = new MPI_Request;
  MPI_Isend(
    _metaDataSizeBuffer, Base::_currentElement, MPI_INT, Base::_ioRank, Base::_ioTag, comm, _metaDataSizeMPIRequest
  );
  logDebug("startSend(int,int)", "sent out size meta data of size " << Base::_currentElement);

#if PeanoDebug >= 1
  _metaDataDebugMPIRequest = new MPI_Request;
  MPI_Isend(
    _metaDataDebugBuffer,
    Base::_currentElement * 2 * Dimensions,
    MPI_DOUBLE,
    Base::_ioRank,
    Base::_ioTag,
    comm,
    _metaDataDebugMPIRequest
  );
  logDebug("startSend(int,int)", "sent out debug meta data of size " << Base::_currentElement * 2 * Dimensions);
#endif

  if (flattenedDataSize > 0) {
    _deepCopyDataBuffer = new typename T::DoFType[flattenedDataSize];
    prepareDataToSendOut();
    logDebug("startSend(int,int)", "created deep copy of all " << entry << " entries in the vector of pointers");

    Base::_ioMPIRequest = new MPI_Request;
    MPI_Isend(
      _deepCopyDataBuffer,
      flattenedDataSize,
      translateContextIntoDatatype<typename T::DoFType>(context),
      Base::_ioRank,
      Base::_ioTag,
      comm,
      Base::_ioMPIRequest
    );
  } else {
    logDebug("startSend(int,int)", "there's no actual user data to send out");
  }
#endif
}


template <class T>
void peano4::stacks::STDVectorOverContainerOfPointers<T>::startReceive(
  peano4::grid::TraversalObserver::SendReceiveContext context, int rank, int tag, MPI_Comm comm, int numberOfElements
) {
#ifdef Parallel
  assertion3(Base::_ioMode == IOMode::None, rank, tag, numberOfElements);
  assertion3(numberOfElements > 0, rank, tag, numberOfElements);

  Base::_ioMode = IOMode::MPIReceive;
  Base::_ioTag  = tag;
  Base::_ioRank = rank;

  assertion(_metaDataSizeMPIRequest == nullptr);
  assertion(_metaDataDebugMPIRequest == nullptr);
  assertion(Base::_ioMPIRequest == nullptr);

  assertion(_deepCopyDataBuffer == nullptr);
  assertion(_metaDataSizeBuffer == nullptr);
  assertion(_metaDataDebugBuffer == nullptr);

  Base::_data.resize(numberOfElements);
  Base::_currentElement = numberOfElements;
  assertionEquals(Base::_data.size(), numberOfElements);

  _context      = context;
  _communicator = comm;

  _metaDataSizeBuffer     = new int[Base::_currentElement];
  _metaDataSizeMPIRequest = new MPI_Request;

#if PeanoDebug >= 1
  _metaDataDebugBuffer     = new double[Base::_currentElement * 2 * Dimensions];
  _metaDataDebugMPIRequest = new MPI_Request;
#endif

  MPI_Irecv(_metaDataSizeBuffer, numberOfElements, MPI_INT, Base::_ioRank, Base::_ioTag, comm, _metaDataSizeMPIRequest);
  logDebug(
    "startReceive(int,int,int)",
    "received all meta data from rank " << rank << " on tag " << tag << " with " << numberOfElements << " entries"
  );

#if PeanoDebug >= 1
  MPI_Irecv(
    _metaDataDebugBuffer,
    numberOfElements * 2 * Dimensions,
    MPI_DOUBLE,
    Base::_ioRank,
    Base::_ioTag,
    comm,
    _metaDataDebugMPIRequest
  );
  logDebug(
    "startReceive(int,int,int)", "trigger receive of debug data with cardinality " << numberOfElements * 2 * Dimensions
  );
#endif

  int metaDataSizeFlag = 1;
  MPI_Test(_metaDataSizeMPIRequest, &metaDataSizeFlag, MPI_STATUS_IGNORE);
  if (metaDataSizeFlag) {
    triggerNonBlockingDataReceive();
  }
#endif
}

template <class T>
bool peano4::stacks::STDVectorOverContainerOfPointers<T>::tryToFinishSendOrReceive() {
#ifdef Parallel
  logTraceInWith4Arguments(
    "tryToFinishSendOrReceive()", ::toString(Base::_ioMode), Base::size(), Base::_ioRank, Base::_ioTag
  );

  bool result = true;

  if (Base::_ioMode == IOMode::MPIReceive) {
    assertion(_metaDataSizeMPIRequest != nullptr);

    int metaDataFlag = 1;
    MPI_Test(_metaDataSizeMPIRequest, &metaDataFlag, MPI_STATUS_IGNORE);

    if (metaDataFlag and _deepCopyDataBuffer == nullptr) {
      triggerNonBlockingDataReceive();
    }

    int metaDataDebugFlag = 1;
    if (_metaDataDebugMPIRequest != nullptr)
      MPI_Test(_metaDataDebugMPIRequest, &metaDataDebugFlag, MPI_STATUS_IGNORE);

    int dataFlag = 1;
    if (Base::_ioMPIRequest != nullptr)
      MPI_Test(Base::_ioMPIRequest, &dataFlag, MPI_STATUS_IGNORE);

    if (metaDataFlag and metaDataDebugFlag and dataFlag) {
      result = true;

      logDebug("tryToFinishSendOrReceive()", "receive complete");

      workInReceivedMetaData();

      if (_metaDataSizeMPIRequest != nullptr) {
        delete _metaDataSizeMPIRequest;
        delete[] _metaDataSizeBuffer;
        _metaDataSizeMPIRequest = nullptr;
        _metaDataSizeBuffer     = nullptr;
      }
      if (_metaDataDebugMPIRequest != nullptr) {
        delete _metaDataDebugMPIRequest;
        delete[] _metaDataDebugBuffer;
        _metaDataDebugMPIRequest = nullptr;
        _metaDataDebugBuffer     = nullptr;
      }
      if (Base::_ioMPIRequest != nullptr) {
        workInReceivedData();

        delete Base::_ioMPIRequest;
        delete[] _deepCopyDataBuffer;
        Base::_ioMPIRequest = nullptr;
        _deepCopyDataBuffer = nullptr;
      }

      Base::_ioMode = IOMode::None;
    } else {
      result = false;
    }
  } else if (Base::_ioMode == IOMode::MPISend) {
    assertion(_metaDataSizeMPIRequest != nullptr);

    int metaDataSizeFlag = 1;
    if (_metaDataSizeMPIRequest != nullptr)
      MPI_Test(_metaDataSizeMPIRequest, &metaDataSizeFlag, MPI_STATUS_IGNORE);

    int metaDataDebugFlag = 1;
    if (_metaDataDebugMPIRequest != nullptr)
      MPI_Test(_metaDataDebugMPIRequest, &metaDataDebugFlag, MPI_STATUS_IGNORE);

    int dataFlag = 1;
    if (Base::_ioMPIRequest != nullptr)
      MPI_Test(Base::_ioMPIRequest, &dataFlag, MPI_STATUS_IGNORE);

    if (metaDataSizeFlag and metaDataDebugFlag and dataFlag) {
      result = true;

      logDebug("tryToFinishSendOrReceive()", "send complete");

      if (_metaDataSizeMPIRequest != nullptr) {
        delete _metaDataSizeMPIRequest;
        delete[] _metaDataSizeBuffer;
        _metaDataSizeMPIRequest = nullptr;
        _metaDataSizeBuffer     = nullptr;
      }
      if (_metaDataDebugMPIRequest != nullptr) {
        delete _metaDataDebugMPIRequest;
        delete[] _metaDataDebugBuffer;
        _metaDataDebugMPIRequest = nullptr;
        _metaDataDebugBuffer     = nullptr;
      }
      if (Base::_ioMPIRequest != nullptr) {
        delete Base::_ioMPIRequest;
        delete[] _deepCopyDataBuffer;
        Base::_ioMPIRequest = nullptr;
        _deepCopyDataBuffer = nullptr;
      }

      Base::clear();
      Base::_ioMode = IOMode::None;
    } else {
      result = false;
    }
  }

  logTraceOutWith1Argument("tryToFinishSendOrReceive()", result);
  return result;
#else
  return true;
#endif
}
