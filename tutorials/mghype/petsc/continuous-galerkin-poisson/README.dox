/**

 @page tutorials_documentation_multigrid_petsc_continuous_galerkin_poisson Continuous Galerkin for the Poisson equation with PETSc

<!-- Add this one for href links to subitems -->
\tableofcontents


 This simple example discusses the steps from the file benchmarks/multigrid/petsc/poisson/fem.py.
 We omit some technical details (such as the parsing of the command line) and 
 focus on key ingredients. Therefore, it is reasonable to study this description 
 in combination with the actual script.
 
 
 # Code usage
  
 ## Create a project
 
 First, we have to create a multigrid project.
 
 ~~~~~~~~~~~~~~~~~~~~~~~~
project = petsc.Project(project_name = "Poisson", 
                        namespace = [ "multigrid", "petsc", "poisson" ]
                        ) 
 ~~~~~~~~~~~~~~~~~~~~~~~~

 This is kind of an empty hull until we befill it with some global information 
 (such as the domain size) and also tell the API to scan the global Makefile.
 
 ~~~~~~~~~~~~~~~~~~~~~~~~
cube_size = 1.0
project.set_global_simulation_parameters(
  dimensions            = 2,
  offset                = [0.0, 0.0, 0.0],
  domain_size           = [ cube_size, cube_size, cube_size],
)

project.set_load_balancing( "toolbox::loadbalancing::strategies::RecursiveSubdivision", "new ::exahype2::LoadBalancingConfiguration()" )
project.set_Peano4_installation( args.peanodir, 
                                 build_mode = peano4.output.CompileMode.Asserts
                                 )
 ~~~~~~~~~~~~~~~~~~~~~~~~
 
 The routine petsc.Project.set_Peano4_installation() will parse all the 
 settings that you used for configure (autotools) or cmake. From hereon,
 these settings will be used by the Python API as well.


 ## Add the solver
 
 We use one of the simplest solvers that we have for the examples here:
 
 ~~~~~~~~~~~~~~~~~~~~~~~~
 solver = petsc.solvers.CollocatedLowOrderDiscretisation("Poisson", 
                                                        1, # a scalar
                                                        0.01,
                                                        0.01
                                                        )

 project.add_solver( solver )
 ~~~~~~~~~~~~~~~~~~~~~~~~

 The solver knows, by definition, which data objects it has to build up 
 internally and to which grid entities these data objects are associated 
 to. As we use a collocated solver here, the scalar is attached to the 
 mesh vertices. The call of petsc.Project.add_solver() will ensure that 
 this solver's data structures are tied to the mesh and that the solver
 enriches all the algorithmic steps with the required operations. 

 ## Generate a Peano project and the C++ code
 
 We close the solver construction by asking the multigrid API to create a 
 Peano 4 Python project for us, and subsequently ask this Peano 4 project
 to generate all C++ code and the Makefile:
 
 ~~~~~~~~~~~~~~~~~~~~~~~~
peano4_project = project.generate_Peano4_project(args.verbose)
peano4_project.generate()
 ~~~~~~~~~~~~~~~~~~~~~~~~
 
 After this last step, we can either invoke peano4.Project.build() on the
 peano4_project object, or we can simply type in make on the command line.
 Before we do this, we have to add the actual numerics:
 
 
 ## Add numerics
  
 Now it is time to build your code. The Python API will never overwrite your
 user code, i.e. alter the Poisson.h and Poisson.cpp file. So you are save
 when you rerun the Python script once more and make it call 
 
        peano4_project.build()
        
 As it is some overhead to regenerate all of that glue code, just type
 in make on the terminal, and everything should be fine. Our implementation
 of the Poisson setup is really simplistic. As we want to solve
 
 @todo Sean can you write down stuff here?
 

 ## Visualisation
 
 Once you have built your code and invoked it, you should see an output similar
 to

 ~~~~~~~~~~~~~~~~~~~~~~~~
00:00:11    rank:0  ::::main()  info    terminated successfully
00:00:11    rank:0  ::::main()  info    grid construction:           5.43634s   (avg=0.339771,#measurements=16,max=0.745847(value #10),min=0.000293452(value #1),+119.514%,-99.9136%,std-deviation=0.316354)
00:00:11    rank:0  ::::main()  info    enumerate:                   0.589898s  (avg=0.589898,#measurements=1,max=0.589898(value #0),min=0.589898(value #0),+0%,-0%,std-deviation=3.4285e-09)
00:00:11    rank:0  ::::main()  info    init (setup PETSc):          4.52e-07s  (avg=4.52e-07,#measurements=1,max=4.52e-07(value #0),min=4.52e-07(value #0),+0%,-0%,std-deviation=3.45865e-15)
00:00:11    rank:0  ::::main()  info    assemble:                    0.591637s  (avg=0.591637,#measurements=1,max=0.591637(value #0),min=0.591637(value #0),+0%,-0%,std-deviation=1.74537e-09)
00:00:11    rank:0  ::::main()  info    solve:                       4.3e-07s   (avg=4.3e-07,#measurements=1,max=4.3e-07(value #0),min=4.3e-07(value #0),+0%,-0%,std-deviation=3.29368e-15)
00:00:11    rank:0  ::::main()  info    map solution back onto mesh: 0.575858s  (avg=0.575858,#measurements=1,max=0.575858(value #0),min=0.575858(value #0),+0%,-0%,std-deviation=3.90214e-09)
00:00:11    rank:0  ::::main()  info    plotting:                    1.03918s   (avg=1.03918,#measurements=1,max=1.03918(value #0),min=1.03918(value #0),+0%,-0%,std-deviation=7.62111e-09)
 ~~~~~~~~~~~~~~~~~~~~~~~~

 Furthermore, there should be patch files in our execution directory. One of 
 them is called solution-Poisson.peano-patch-file and it is a meta file, i.e.
 it links to all the other files written by different ranks and threads on 
 your parallel computer. You can inspect patch files in a text editor, and 
 their format @ref peano_patch_file "is described in the patch file format section".
 
 To visualise the outcome, you can go down different paths. I have pvpython 
 installed, i.e. Paraview with the Python terminal. In this case, we can type 
 in 
 
 ~~~~~~~~~~~~~~~~~~~~~~~~
 /opt/paraview/bin/pvpython ~/git/Peano/python/peano4/visualisation/render.py --filter-fine-grid solution-Poisson.peano-patch-file
 ~~~~~~~~~~~~~~~~~~~~~~~~

 which converts the patch files into a pvd file which we can open in Paraview.
  
  
  ## What happens under the hood

  The multigrid project will yield a Peano project which in turn will create
  
  - all the C++ code;
  - a main file;
  - a Makefile
  
  The main file realises one by one the different steps of a PETSc-based multigrid
  solver and therefore follows exactly @ref peano_main "the vanilla Peano main file structure".
  It runs through the algorithmic steps   
  Per algorithmic step that we @ref documentation_multigrid_petsc "find in any PETSc" project one by one.
  Per step, it issues one mesh traversal (besides for the actual solve, which is a single PETSc
  call).
  Per traversal, it hands in an observer.
  
  The observers are created by the multigrid project petsc.Project. This class
  speaks of steps which is really what they are. The observer is the realisation
  of these steps.
  The steps that petsc.Project creates are more or less empty. 
  When we invoke petsc.Project.add_solver(), the project takes the solver as passed
  as an argument and asks the solver to add action sets, i.e. activities, to each 
  step. Each activity is a subclass of peano4.solversteps.ActionSet.

  @see peano4.petsc.actionsets.InitVertexDoFs for the actual initialisation phase.

  ## Libraries

  We compile with flag -lpetsc to ensure that the PETSc-specific libraries are available to us.
  (not to be confused with -lPETSc). To ensure these are visible to the compiler, we must add
  
  ~~~~~~~~~~~~~~~~~~~~~~~~
  -I$PETSC_DIR/include -I$PETSC_DIR/$PETSC_ARCH/include
  ~~~~~~~~~~~~~~~~~~~~~~~~

  to the CXXFLAGS. This assumes that PETSc is installed locally, and these environment variables
  are set up. Furthermore. we add the following to the LDFLAGS:

  ~~~~~~~~~~~~~~~~~~~~~~~~
  -Wl,-rpath,$PETSC_DIR/$PETSC_ARCH/lib -L$PETSC_DIR/$PETSC_ARCH/lib -L/usr/lib/gcc/x86_64-linux-gnu
  ~~~~~~~~~~~~~~~~~~~~~~~~

  At time of writing, these can be seen in lines 788-815 of configure.ac, at the top level of Peano.

  # Internal workflow

  During EnumerateAndInit stage, we request one index in the LocalToGlobalMap for each
  degree of freedom. Each time this occurs, we increase the index count on each subtree.
  At the end of the traversal, we merge the LocalToGlobalMaps from each subtree back into
  the main map. We keep a std::map which tracks how many indices were used up by each subtree,
  so we can issue global Indices sequentially. 

  During InitVertexDof stage, we store each Value and Rhs that we encounter. 
  When it comes to the InitPetsc stage, we send these values to PetscData class, 
  which then uses these values to initialise two Petsc vectors. So far, we have implemented 
  a method to return the values from the solution vector from PETSc back to C++.



This is a brief discussion of key ideas behind the Continuous Galerkin method, 
i.e. what most people learn as standard Finite Elements in lectures. 
While we discuss some of the mathematics behind FEM, the focus
of this text is informal (from a maths point of view) but puts special emphasis
on how numerical concepts are mapped onto Peano's data structures and mesh 
traversal. 


# Method

@todo This has to be reworked almost from scratch

# Example: the Poisson equation

@todo This has to be reworked almost from scratch
 
As ever we aim to solve

@f$ -\Delta u \ = \ f @f$.

And we integrate against a test function @f$ \phi @f$. In brief, we pick our solution
@f$ u @f$ to be a member of some space of polynomials, and (importantly) we pick
the test function @f$ \phi @f$ to come from the same space:

@f$ \int (-\Delta u, \ \phi) dx \ = \  \int (f, \ \phi) dx  \ \ \ \forall \phi@f$.

For emphasis, we want the above equation to hold for every test function @f$ \phi @f$ in our function space.

## Some example

Let's attempt make this concrete, by way of an example. We depict in the image below
the following situation:

- A 1D domain, cut up into discrete chunks of width @f$ h @f$.
- We pick test functions @f$ \phi @f$ that are piecewise linear, and each span two cells. Furthermore normalised to a maximum height of 1.
- We have a value for @f$ u @f$ at the boundary of each cell.

What we aim to do here is to show how we update @f$ u @f$, based on our formulation.

@image html continuous_shape_functions.png

From here onwards, we pick our @f$ u @f$ to be a member of some space of polynomials, and we label the coefficients/weights
as the @f$ u_i @f$. Concretely:

@f$ u = \sum_i u_i \ \phi_i @f$.


Returning to the integral above, we can integrate by parts to obtain

\f{eqnarray*}{
  &\int_\text{cell}& ( \nabla u, \ \nabla \phi )dx + \int_\text{face} (\nabla u \cdot \hat{n}, \ \phi) dS(x)
   \ \ \ \forall \phi \\
  =\sum_i u_i \ &\int_\text{cell}& (\nabla \phi_i, \ \nabla \phi) dx 
  + \int_\text{face} (\nabla u \cdot \hat{n}, \ \phi) dS(x)
  \ \ \ \forall \phi

\f}

where in the second line we exchanged the definition of @f$ u @f$ for its expansion
in basis functions. From here onwards, we drop the surface integral term, for reasons we shall 
explain later. 

Next, let's discretise. Rather than taking an integral across the whole domain, we work cell-by-cell. We label each cell with index @f$ k @f$:

\f{eqnarray*}{
  = \sum_{i, \ k} \ \int_{c_k} \ (\nabla \phi_i, \ \nabla \phi) dx \ \ \ \forall \phi
\f}

## A Galerkin Finite Element solver for the Poisson equation with PETSc

@todo Sean, your turn



*/


<!-- 
WE COMMENT THIS OUT. I DON'T THINK IT'S READY YET

Now we are working on a cell-by-cell basis, let us examine the @f$ 1^{st} @f$ cell.
We note that only @f$ \phi_0 @f$ and @f$ \phi_1 @f$ are non-zero here, so our original equation reduces to:

\f{eqnarray*}{
 u_1 \ \sum_{k=1,2} \int_{c_k} (\nabla \phi_0, \ \phi_0) \ + \
  
   dx \ = \ 
  \int_{c_0} (f, \ \phi_0) dx
\f}

and we drop the @f$ \forall \phi @f$ requirement since only @f$ \phi_0 @f$ is non-zero here. In effect, this

@f$ \int_{c_0} (\nabla \phi_0, \ \phi_0) dx \ = \ \int_{c_0} (f, \ \phi_0) dx @f$ 

will become our matrix element, as we shall see shortly.
-->


## The nodal basis 

The basis of functions that we use in this example are piecewise linear functions, defined in 
each cell (which is an interval of width @f$ h @f$):

\f{eqnarray*}{
  \phi_i(x) \ = \
\begin{cases}
  \frac{1}{h}(x - x_{i-1}) &\text{if} \ x_{i-1} \ < \ x < \ x_i \\
  \frac{1}{h}(x_{i+1} - x) &\text{if} \ x_{i} \ < \ x < \ x_{i+1} \\
  0                        &\text{otherwise}
\end{cases}
\f}

And from this we can write down @f$ \nabla \phi_i @f$:

\f{eqnarray*}{
  \nabla \phi_i(x) \ = \
\begin{cases}
  \frac{1}{h} &\text{if} \ x_{i-1} \ < \ x < \ x_i \\
  - \frac{1}{h} &\text{if} \ x_{i} \ < \ x < \ x_{i+1} \\
  0                        &\text{otherwise}
\end{cases}
\f}

And finally compute the overlap integrals. Here we integrate over the entire domain (every cell):

\f{eqnarray*}{

\int (\nabla \phi_i, \ \nabla \phi_i) dx \ &=& \ \int_{c_{i}} \frac{1}{h} \cdot \frac{1}{h} dx
    + \int_{c_{i+1}} \frac{-1}{h} \cdot \frac{-1}{h} dx \ = \ \frac{2}{h} \\
\int (\nabla \phi_i, \ \nabla \phi_{i+1}) dx \ &=& \ \int_{c_{i+1}} \frac{-1}{h} \cdot \frac{1}{h} dx \ = \ \frac{1}{h}


\f}

With all other being 0, due to the lack of overlap. We used here implicitly that the cell had a width of @f$ h @f$.

## Where does the stencil notation come from?

Explain here how the stencil is just one standalone test.
Further explain how we can evaluate a stencil for the rhs by taking
@f$ f = f_1 \phi_1 + f_2 \phi_2 @f$


## Constructing our matrix elements

Now it's time to properly construct our matrix. Most linear algebra problems state problems in the 
form @f$ Au \ = \ f @f$, for some known @f$ A @f$ and @f$ f @f$, and it is our job to work
out what @f$ u @f$ should be. We start off by fixing @f$ u_0 \ \text{and} \ u_{n+1} @f$ to be 0 
(we use Dirichlet boundary conditions).

We summarise some key points:

\li @f$ u_1 @f$ and @f$ u_2 @f$ lie at our end points
\li Only @f$\phi_1@f$ and @f$\phi_2@f$ are non-zero here 

<!--
\f{eqnarray*}{

  \int_{c_1} (\nabla u, \ \nabla \phi) dx \ &=& \int_{c_1} (f, \phi ) \ \ \ \forall \phi

\f}

-->

### The j = 1 Element
We want the equation above to hold for all @f$ \phi @f$ indeed, but since we are constructing matrix elements,
let's investigate the @f$ j \ = \ 1 @f$ element first. 


\f{eqnarray*}{
  \int (\nabla u, \ \nabla \phi_1) dx &=& \int (f, \phi_1) dx \\
  \int (u_i \sum \nabla \phi_i, \ \nabla \phi_1) dx &=& \int (f, \phi_1) dx \\
\f}

We note that @f$ \phi_1 @f$ only has overlap with @f$ \phi_2 @f$ and itself:

\f{eqnarray*}{
 
  \int (u_1 \nabla \phi_1, \ \nabla \phi_1) dx +
  \int (u_2 \nabla \phi_2, \ \nabla \phi_1) dx 
    &=& \int (f, \phi_1) dx \\

  \frac{2}{h} u_1 \ - \  \frac{1}{h}u_2 &=& \int (f, \phi_1) dx
\f}


### The j = 2 Element

Let's do the same thing again. This time we have overlap with @f$ \phi_1 @f$ and @f$ \phi_3 @f$:

\f{eqnarray*}{
  \int (\nabla u, \ \nabla \phi_2) dx &=& \int (f, \phi_2) dx \\
  \int (u_i \sum \nabla \phi_i, \ \nabla \phi_2) dx &=& \int (f, \phi_2) dx \\
  \int (u_1 \nabla \phi_1, \ \nabla \phi_1) dx +
  \int (u_2 \nabla \phi_2, \ \nabla \phi_2) dx +
  \int (u_2 \nabla \phi_2, \ \nabla \phi_3) dx 
    &=& \int (f, \phi_2) dx \\

  -\frac{1}{h} u_1 \ + \frac{2}{h} u_2 \ - \  \frac{1}{h}u_3 &=& \int (f, \phi_2) dx
  
\f}

We can see here where the well-known 1D Poisson stencil @f$ \frac{1}{h} [-1, 2, -1] @f$ comes from.

##The Problem Matrix
And we can thus start constructing our problem matrix:

\f{eqnarray*}{

  &\begin{pmatrix}
  \int (\nabla \phi_1, \ \nabla \phi_1) dx & \int (\nabla \phi_2, \ \nabla \phi_1) dx & 0 & 0  & ... \\
  \int (\nabla \phi_1, \ \nabla \phi_2) dx & \int (\nabla \phi_2, \ \nabla \phi_2) dx & \int (\nabla \phi_3, \ \nabla \phi_2) dx & 0  & ... \\
  ... & ... & ... & ... \\
  \end{pmatrix}

  \begin{pmatrix}
  u_1 \\
  u_2 \\
  u_3 \\
  u_4\\
  ... 
  \end{pmatrix} \\

= \frac{1}{h} &\begin{pmatrix}
  2 & -1 & 0 & 0 & ... \\
  -1 & 2 & -1 & 0 & ... \\
\end{pmatrix}

  \begin{pmatrix}
  u_1 \\
  u_2 \\
  u_3 \\
  u_4\\
  ... 
  \end{pmatrix} \\

\f}

In particular, our matrix elements will be:

\f{eqnarray*}{

A_{ij} = \int (\nabla \phi_j, \ \nabla \phi_i) dx 

\f}

By construction, this will be a sparse matrix, as these basis functions are chosen to not overlap
with each other almost everywhere. In other words, they have only local support.

## Constructing our right-hand-side

We can read off the right hand side values, by taking our known right-hand-side @f$ f @f$ and 
integrating against an appropriate test function:

\f{eqnarray*}{
  f_i = \int (f, \phi_i) dx

\f}

Please excuse the slight abuse of notation here. There should probably be a clearer indication that
we move from a known, continuous @f$ f @f$ to some discrete @f$ f_i @f$.

## Why did we drop the surface integral?

We can demonstrate simply. It's worth doing, as when we discuss the discontinuous version of this
scenario, it will no longer disappear.

We take our left-hand-side and discretise it; we integrate over each cell and then take the sum 
across each of the cells. We also consider the terms that enter the @f$ k^{th} @f$ row, so we 
are integrating with @f$ \phi_k @f$:

\f{eqnarray*}{
  \int (-\Delta u, \ \phi_k) dx 
    = \sum_j &\int_{c_j}& (\nabla u, \ \nabla \phi_k) dx \\
    - &\int_{\partial c_j^-}& (\nabla u \cdot \hat{n_{j_-}}, \ \phi_k) dS(c_j^-) \\
    - &\int_{\partial c_j^+}& (\nabla u \cdot \hat{n_{j_+}}, \ \phi_k) dS(c_j^+)
\f}

### What do all these terms mean? 

\li @f$ \partial c_j^\pm @f$ denotes the right and left boundaries of the cell, respectively.
\li @f$ \hat{n_{j_\pm}} @f$ denotes the vector that is normal to the cell at the right and left boundaries respectively.

We can afford to lose these last two terms, since they will both appear in consecutive terms of 
the sum across f$ j @f$, but with opposite signs; 
@f$ \hat{n_{j_+}} \ = \ -\hat{n_{{n+1}_-}} @f$,
hence each of these terms will cancel out in a telescopic fashion.

  

*/

 